{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models\n",
    "\n",
    "Play each game with each model and save the results to a csv file. This file can be used to create plots and compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import re\n",
    "from builtins import range\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the TensorFlow logging level to suppress debug messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import Training.TrainingScript as TrainingScript\n",
    "from Ensemble import EnsembleMethods, Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_CSV = \"./../../results.csv\"\n",
    "EVALUATION_RESULTS_CSV = \"./../../evaluation_results.csv\"\n",
    "\n",
    "NUM_REPETITIONS = 15\n",
    "NUM_REPETITIONS_FOR_RANDOM_BASELINE = 250\n",
    "\n",
    "N_FOR_TOP_N_ENSEMBLES = 3\n",
    "M_FOR_SNAPSHOT_ENSEMBLES_AND_SOUPS = 3\n",
    "ENSEMBLE_METHODS_USED = [\n",
    "    EnsembleMethods.AVERAGE,\n",
    "    EnsembleMethods.LOGISTIC_AVERAGE,\n",
    "    EnsembleMethods.AVERAGE_WITH_CONFIDENCE,\n",
    "    EnsembleMethods.LOGISTIC_AVERAGE_WITH_CONFIDENCE,\n",
    "    EnsembleMethods.MAJORITY_VOTE,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>training_model</th>\n",
       "      <th>reward_history</th>\n",
       "      <th>loss_history</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breakout</td>\n",
       "      <td>with_huber_loss_and_adam</td>\n",
       "      <td>[0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 6.0, 0.0, ...</td>\n",
       "      <td>[0.01637558452785015, 0.011548655107617378, 0....</td>\n",
       "      <td>./../models/with_huber_loss_and_adam/breakout/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>breakout</td>\n",
       "      <td>with_huber_loss_and_adam</td>\n",
       "      <td>[2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, ...</td>\n",
       "      <td>[0.0017978374380618334, 0.008414973504841328, ...</td>\n",
       "      <td>./../models/with_huber_loss_and_adam/breakout/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enduro</td>\n",
       "      <td>with_huber_loss_and_adam</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.013470819219946861, 0.00460647139698267, 0....</td>\n",
       "      <td>./../models/with_huber_loss_and_adam/enduro/st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2015</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.004595236387103796, 0.005559221841394901, 0...</td>\n",
       "      <td>./../models/mnih2015/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.10596290975809097, 0.11753647774457932, 0.1...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       game            training_model  \\\n",
       "0  breakout  with_huber_loss_and_adam   \n",
       "1  breakout  with_huber_loss_and_adam   \n",
       "2    enduro  with_huber_loss_and_adam   \n",
       "3    enduro                  mnih2015   \n",
       "4    enduro                  mnih2013   \n",
       "\n",
       "                                      reward_history  \\\n",
       "0  [0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 6.0, 0.0, ...   \n",
       "1  [2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                        loss_history  \\\n",
       "0  [0.01637558452785015, 0.011548655107617378, 0....   \n",
       "1  [0.0017978374380618334, 0.008414973504841328, ...   \n",
       "2  [0.013470819219946861, 0.00460647139698267, 0....   \n",
       "3  [0.004595236387103796, 0.005559221841394901, 0...   \n",
       "4  [0.10596290975809097, 0.11753647774457932, 0.1...   \n",
       "\n",
       "                                          model_path  \n",
       "0  ./../models/with_huber_loss_and_adam/breakout/...  \n",
       "1  ./../models/with_huber_loss_and_adam/breakout/...  \n",
       "2  ./../models/with_huber_loss_and_adam/enduro/st...  \n",
       "3  ./../models/mnih2015/enduro/started_at_2023-09...  \n",
       "4  ./../models/mnih2013/enduro/started_at_2023-09...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(RESULTS_CSV)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>training_model</th>\n",
       "      <th>reward_history</th>\n",
       "      <th>loss_history</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.10596290975809097, 0.11753647774457932, 0.1...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.03629424795508385, 0.07472917437553406, 0.0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 3.0, 4.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, ...</td>\n",
       "      <td>[0.01985565945506096, 0.01905544474720955, 0.0...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 5.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.001379762077704072, 0.0012381058186292648, ...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 3.0, 1.0, 3.0, 1.0, ...</td>\n",
       "      <td>[0.015244378708302975, 0.014382739551365376, 0...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.010004216805100441, 0.012425143271684647, 0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[40.0, 0.0, 140.0, 60.0, 0.0, 20.0, 20.0, 0.0,...</td>\n",
       "      <td>[0.02893028035759926, 0.027503740042448044, 0....</td>\n",
       "      <td>./../models/mnih2013/seaquest/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[100.0, 40.0, 0.0, 40.0, 0.0, 40.0, 40.0, 40.0...</td>\n",
       "      <td>[0.05895446613430977, 0.05621371418237686, 25....</td>\n",
       "      <td>./../models/mnih2013/seaquest/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[1.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, ...</td>\n",
       "      <td>[0.0011551892384886742, 0.0012054498074576259,...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[1.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 5.0, ...</td>\n",
       "      <td>[0.04612644761800766, 0.04612863063812256, 0.0...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[120.0, 140.0, 20.0, 40.0, 240.0, 60.0, 160.0,...</td>\n",
       "      <td>[12.697918891906738, 12.673356056213379, 0.018...</td>\n",
       "      <td>./../models/mnih2013/seaquest/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 4.0, 1.0, 6.0, 0.0, 2.0, 0.0, 3.0, ...</td>\n",
       "      <td>[0.03820963203907013, 0.004104000050574541, 0....</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0358581505715847, 0.026829775422811508, 0.0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0066382684744894505, 0.005285170860588551, ...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.04718456044793129, 0.03172334283590317, 0.0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[100.0, 0.0, 100.0, 100.0, 0.0, 120.0, 40.0, 2...</td>\n",
       "      <td>[0.012014569714665413, 12.65832805633545, 0.01...</td>\n",
       "      <td>./../models/mnih2013/seaquest/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.021065175533294678, 0.022660288959741592, 0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 40.0, 20.0, 20.0, 20.0, 120.0, 160.0, 20...</td>\n",
       "      <td>[0.02780219167470932, 0.027884984388947487, 0....</td>\n",
       "      <td>./../models/mnih2013/seaquest/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 2.0, 0.0, 2.0, 6.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.017719639465212822, 0.016713574528694153, 0...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>enduro</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.042389389127492905, 0.031015925109386444, 0...</td>\n",
       "      <td>./../models/mnih2013/enduro/started_at_2023-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>[0.043728306889534, 0.03957980498671532, 0.035...</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>breakout</td>\n",
       "      <td>mnih2013</td>\n",
       "      <td>[0.0, 2.0, 0.0, 0.0, 3.0, 0.0, 2.0, 0.0, 3.0, ...</td>\n",
       "      <td>[0.04034668952226639, 0.004771257750689983, 0....</td>\n",
       "      <td>./../models/mnih2013/breakout/started_at_2023-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        game training_model  \\\n",
       "4     enduro       mnih2013   \n",
       "5     enduro       mnih2013   \n",
       "10  breakout       mnih2013   \n",
       "12  breakout       mnih2013   \n",
       "14  breakout       mnih2013   \n",
       "18    enduro       mnih2013   \n",
       "30  seaquest       mnih2013   \n",
       "31  seaquest       mnih2013   \n",
       "32  breakout       mnih2013   \n",
       "33  breakout       mnih2013   \n",
       "34  seaquest       mnih2013   \n",
       "35  breakout       mnih2013   \n",
       "36    enduro       mnih2013   \n",
       "37    enduro       mnih2013   \n",
       "38    enduro       mnih2013   \n",
       "50  seaquest       mnih2013   \n",
       "53    enduro       mnih2013   \n",
       "54  seaquest       mnih2013   \n",
       "55  breakout       mnih2013   \n",
       "56    enduro       mnih2013   \n",
       "57  breakout       mnih2013   \n",
       "58  breakout       mnih2013   \n",
       "\n",
       "                                       reward_history  \\\n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "10  [0.0, 3.0, 4.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, ...   \n",
       "12  [0.0, 5.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "14  [0.0, 1.0, 0.0, 0.0, 0.0, 3.0, 1.0, 3.0, 1.0, ...   \n",
       "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "30  [40.0, 0.0, 140.0, 60.0, 0.0, 20.0, 20.0, 0.0,...   \n",
       "31  [100.0, 40.0, 0.0, 40.0, 0.0, 40.0, 40.0, 40.0...   \n",
       "32  [1.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, ...   \n",
       "33  [1.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 5.0, ...   \n",
       "34  [120.0, 140.0, 20.0, 40.0, 240.0, 60.0, 160.0,...   \n",
       "35  [0.0, 0.0, 4.0, 1.0, 6.0, 0.0, 2.0, 0.0, 3.0, ...   \n",
       "36  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "37  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "38  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "50  [100.0, 0.0, 100.0, 100.0, 0.0, 120.0, 40.0, 2...   \n",
       "53  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "54  [0.0, 40.0, 20.0, 20.0, 20.0, 120.0, 160.0, 20...   \n",
       "55  [0.0, 0.0, 2.0, 0.0, 2.0, 6.0, 1.0, 1.0, 1.0, ...   \n",
       "56  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "57  [1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, ...   \n",
       "58  [0.0, 2.0, 0.0, 0.0, 3.0, 0.0, 2.0, 0.0, 3.0, ...   \n",
       "\n",
       "                                         loss_history  \\\n",
       "4   [0.10596290975809097, 0.11753647774457932, 0.1...   \n",
       "5   [0.03629424795508385, 0.07472917437553406, 0.0...   \n",
       "10  [0.01985565945506096, 0.01905544474720955, 0.0...   \n",
       "12  [0.001379762077704072, 0.0012381058186292648, ...   \n",
       "14  [0.015244378708302975, 0.014382739551365376, 0...   \n",
       "18  [0.010004216805100441, 0.012425143271684647, 0...   \n",
       "30  [0.02893028035759926, 0.027503740042448044, 0....   \n",
       "31  [0.05895446613430977, 0.05621371418237686, 25....   \n",
       "32  [0.0011551892384886742, 0.0012054498074576259,...   \n",
       "33  [0.04612644761800766, 0.04612863063812256, 0.0...   \n",
       "34  [12.697918891906738, 12.673356056213379, 0.018...   \n",
       "35  [0.03820963203907013, 0.004104000050574541, 0....   \n",
       "36  [0.0358581505715847, 0.026829775422811508, 0.0...   \n",
       "37  [0.0066382684744894505, 0.005285170860588551, ...   \n",
       "38  [0.04718456044793129, 0.03172334283590317, 0.0...   \n",
       "50  [0.012014569714665413, 12.65832805633545, 0.01...   \n",
       "53  [0.021065175533294678, 0.022660288959741592, 0...   \n",
       "54  [0.02780219167470932, 0.027884984388947487, 0....   \n",
       "55  [0.017719639465212822, 0.016713574528694153, 0...   \n",
       "56  [0.042389389127492905, 0.031015925109386444, 0...   \n",
       "57  [0.043728306889534, 0.03957980498671532, 0.035...   \n",
       "58  [0.04034668952226639, 0.004771257750689983, 0....   \n",
       "\n",
       "                                           model_path  \n",
       "4   ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "5   ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "10  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "12  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "14  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "18  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "30  ./../models/mnih2013/seaquest/started_at_2023-...  \n",
       "31  ./../models/mnih2013/seaquest/started_at_2023-...  \n",
       "32  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "33  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "34  ./../models/mnih2013/seaquest/started_at_2023-...  \n",
       "35  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "36  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "37  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "38  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "50  ./../models/mnih2013/seaquest/started_at_2023-...  \n",
       "53  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "54  ./../models/mnih2013/seaquest/started_at_2023-...  \n",
       "55  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "56  ./../models/mnih2013/enduro/started_at_2023-09...  \n",
       "57  ./../models/mnih2013/breakout/started_at_2023-...  \n",
       "58  ./../models/mnih2013/breakout/started_at_2023-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df[\"training_model\"] == \"mnih2013\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breakout', 'enduro', 'seaquest'] ['with_huber_loss_and_adam', 'mnih2015', 'mnih2013', 'interpretable_cnn']\n"
     ]
    }
   ],
   "source": [
    "list_of_games = list(results_df[\"game\"].unique())\n",
    "list_of_algorithms = list(results_df[\"training_model\"].unique())\n",
    "\n",
    "print(list_of_games, list_of_algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data_df = pd.DataFrame(columns=[\"game\", \"model\", \"model_id\", \"episode_rewards\", \"mean\", \"standard_deviation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(autograph=False)\n",
    "def get_action_from_model(model, state):\n",
    "    q_values = model(state)\n",
    "    return tf.argmax(q_values, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(game: str, model: tf.keras.Model, num_repetitions: int = 10):\n",
    "    env = TrainingScript.create_env(game)\n",
    "    # env = gym.wrappers.RecordVideo(env, video_folder='./video/', episode_trigger=lambda episode_id: episode_id % num_repetitions == 0)\n",
    "    rewards = []\n",
    "    for i in (tbar := tqdm(range(num_repetitions), leave=False)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            if random.random() < 0.05:\n",
    "                # To help the agent when it gets \"stuck\"\n",
    "                # Happens mainly in Breakout caused by a bug in the game\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                step += 1\n",
    "                state = tf.convert_to_tensor(state, dtype=tf.float32) / 255.0\n",
    "                state = tf.expand_dims(state, axis=0)\n",
    "                action = get_action_from_model(model, state)\n",
    "                action = action.numpy()[0]\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            tbar.set_description(f\"Episode {i}  -  Step: {step}, Reward: {episode_reward}\")\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: str):\n",
    "    model = tf.keras.models.load_model(path, compile=False)\n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(path_list: list[Path]) -> list:\n",
    "    return [load_model(path) for path in path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(game: str, num_repetitions: int = 5):\n",
    "    env = TrainingScript.create_env(game)\n",
    "    rewards = []\n",
    "\n",
    "    for _ in tqdm(range(num_repetitions), unit=\"episode\", desc=\"Random play \" + game.capitalize(), leave=False):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            step += 1\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshots(dir) -> list:\n",
    "    if os.path.isfile(dir):\n",
    "        dir = os.path.dirname(dir)\n",
    "\n",
    "    snapshots = dict()\n",
    "    snapshot_re = re.compile(r\"snapshot_(?P<idx>\\d+)\\.keras\")\n",
    "\n",
    "    for file in Path(dir).glob(\"*.keras\"):\n",
    "        f_name = str(file.name)\n",
    "        if snapshot_re.match(f_name):\n",
    "            snapshots[int(snapshot_re.match(f_name)[\"idx\"])] = str(file)\n",
    "\n",
    "    snapshots[len(snapshots)] = str(Path(dir) / \"snapshot_final.keras\")\n",
    "    sorted_snapshots = list(zip(*sorted(list(snapshots.items()), key=lambda a: a[1])))[1]\n",
    "\n",
    "    return sorted_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline for Each Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1d00e5870247469e3c2668dd90dd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculate random baseline:   0%|          | 0/3 [00:00<?, ?game/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b0b2cf6754bf49d0e2ba2c25b9801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random play Breakout:   0%|          | 0/250 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d206c66bdea42ad822bd6c8f42c5439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random play Enduro:   0%|          | 0/250 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97526a2682554dac98e8fea4190a7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random play Seaquest:   0%|          | 0/250 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_baseline = []\n",
    "for game_name in tqdm(results_df[\"game\"].unique(), unit=\"game\", desc=\"Calculate random baseline\"):\n",
    "    rewards = random_play(game_name, num_repetitions=NUM_REPETITIONS_FOR_RANDOM_BASELINE)\n",
    "    random_baseline.append(\n",
    "        {\n",
    "            \"game\": game_name,\n",
    "            \"model\": \"random play\",\n",
    "            \"model_id\": 0,\n",
    "            \"episode_rewards\": rewards,\n",
    "            \"mean\": np.mean(rewards),\n",
    "            \"standard_deviation\": np.std(rewards)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>model</th>\n",
       "      <th>model_id</th>\n",
       "      <th>episode_rewards</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breakout</td>\n",
       "      <td>random play</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.264</td>\n",
       "      <td>1.397964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enduro</td>\n",
       "      <td>random play</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seaquest</td>\n",
       "      <td>random play</td>\n",
       "      <td>0</td>\n",
       "      <td>[20.0, 80.0, 160.0, 120.0, 0.0, 100.0, 120.0, ...</td>\n",
       "      <td>80.160</td>\n",
       "      <td>61.812413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       game        model  model_id  \\\n",
       "0  breakout  random play         0   \n",
       "1    enduro  random play         0   \n",
       "2  seaquest  random play         0   \n",
       "\n",
       "                                     episode_rewards    mean  \\\n",
       "0  [0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, ...   1.264   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.000   \n",
       "2  [20.0, 80.0, 160.0, 120.0, 0.0, 100.0, 120.0, ...  80.160   \n",
       "\n",
       "   standard_deviation  \n",
       "0            1.397964  \n",
       "1            0.000000  \n",
       "2           61.812413  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_baseline_df = pd.DataFrame.from_records(random_baseline)\n",
    "random_baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate each Model on each Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828e20b946d4105b304b82391777c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at C:\\Users\\timwi\\Documents\\Uni\\DeepReinforcementLearning\\src\\models\\with_huber_loss_and_adam\\breakout\\started_at_2023-09-23_14-53-39\\snapshot_final.keras",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, m_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fitting_models):\n\u001b[0;32m      9\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(m_path)\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m evaluate_model(game_name, model, num_repetitions\u001b[38;5;241m=\u001b[39mNUM_REPETITIONS)\n\u001b[0;32m     14\u001b[0m     evaluation_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgame\u001b[39m\u001b[38;5;124m\"\u001b[39m: game_name,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: m_path\n\u001b[0;32m     22\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepReinforcementLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepReinforcementLearning\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at C:\\Users\\timwi\\Documents\\Uni\\DeepReinforcementLearning\\src\\models\\with_huber_loss_and_adam\\breakout\\started_at_2023-09-23_14-53-39\\snapshot_final.keras"
     ]
    }
   ],
   "source": [
    "evaluation_results = []\n",
    "for game_name, model_name in (t := tqdm(list(itertools.product(list_of_games, list_of_algorithms)))):\n",
    "    t.set_description(f\"Evaluation models for {game_name.capitalize()} using {model_name.capitalize()}\")\n",
    "    fitting_models = list(\n",
    "        results_df[(results_df[\"game\"] == game_name) & (results_df[\"training_model\"] == model_name)][\"model_path\"]\n",
    "    )\n",
    "\n",
    "    for idx, m_path in enumerate(fitting_models):\n",
    "        m = str(Path(m_path).resolve())\n",
    "\n",
    "        model = load_model(m)\n",
    "        rewards = evaluate_model(game_name, model, num_repetitions=NUM_REPETITIONS)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"game\": game_name,\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": idx,\n",
    "            \"episode_rewards\": rewards,\n",
    "            \"mean\": np.mean(rewards),\n",
    "            \"standard_deviation\": np.std(rewards),\n",
    "            \"model_path\": m_path\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_single_models_df = pd.DataFrame.from_records(evaluation_results)\n",
    "evaluation_single_models_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate as Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_used = set(list_of_algorithms) - {\"random play\", \"interpretable_cnn\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(game: str, model: tf.keras.Model, num_repetitions: int = 10):\n",
    "    env = TrainingScript.create_env(game)\n",
    "    rewards = []\n",
    "    for i in (tbar := tqdm(range(num_repetitions), leave=False)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            if random.random() < 0.05:\n",
    "                # To help the agent when it gets \"stuck\"\n",
    "                # Happens mainly in Breakout caused by a bug in the game\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                step += 1\n",
    "                state = tf.convert_to_tensor(state, dtype=tf.float32) / 255.0\n",
    "                state = tf.expand_dims(state, axis=0)\n",
    "                q_values = model.predict(state, verbose=0)\n",
    "                action = tf.argmax(q_values, axis=1)\n",
    "                action = action.numpy()[0]\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            tbar.set_description(f\"Episode {i}  -  Step: {step}, Reward: {episode_reward}\")\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Ensembles consisting of the best $n$ models of a flavor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_uniform_ensembles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_name, model_name in (t := tqdm(list(itertools.product(list_of_games, models_used)),\n",
    "                                        desc=\"Build and Ensemble Models\")):\n",
    "    t.set_description(f\"Build and Ensemble Models ({model_name.capitalize()} for {game_name})\")\n",
    "\n",
    "    # Select Top-N models:\n",
    "    model_selection = evaluation_single_models_df[\n",
    "                          (evaluation_single_models_df[\"model\"] == model_name) & (evaluation_single_models_df[\"game\"] == game_name)\n",
    "                      ].sort_values(by=[\"mean\", \"standard_deviation\"], ascending=[False, True])[:N_FOR_TOP_N_ENSEMBLES]\n",
    "    model_paths = [str(Path(m_path).resolve()) for m_path in list(model_selection[\"model_path\"])]\n",
    "    models = load_models(model_paths)\n",
    "\n",
    "    for idx, ensemble_method in enumerate(ENSEMBLE_METHODS_USED):\n",
    "        ensemble = Ensemble(models, ensemble_method)\n",
    "        t.set_description(\n",
    "            f\"Build and Ensemble Models ({model_name.capitalize()} for {game_name} with {ensemble_method})\")\n",
    "\n",
    "        print(f\"Build and Ensemble Models ({model_name.capitalize()} for {game_name} with {ensemble_method})\")\n",
    "        rewards = evaluate_ensemble(game_name, ensemble, NUM_REPETITIONS)\n",
    "        evaluation_results_uniform_ensembles.append({\n",
    "            \"game\": game_name,\n",
    "            \"model\": f\"Top-{N_FOR_TOP_N_ENSEMBLES} Ensemble ({ensemble_method}) with {model_name}\",\n",
    "            \"model_id\": idx,\n",
    "            \"episode_rewards\": rewards,\n",
    "            \"mean\": np.mean(rewards),\n",
    "            \"standard_deviation\": np.std(rewards),\n",
    "            \"model_path\": model_paths\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_uniform_ensembles_df = pd.DataFrame.from_records(evaluation_results_uniform_ensembles)\n",
    "evaluation_results_uniform_ensembles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Ensemble\n",
    "\n",
    "One Ensemble containing the best $n$ models of each model type (e.g. 3 models of Mnih 2013, 3 models of Mnih 2015, and 3 models of Mnih 2015 with Huber loss and Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_mixed_ensembles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_name in (t := tqdm(list_of_games)):\n",
    "    t.set_description(f\"Evaluate mixed Ensemble for {game_name}\")\n",
    "    # Select Top-N models for each model type:\n",
    "    model_paths = []\n",
    "    for model_name in list_of_algorithms:\n",
    "        model_selection = evaluation_single_models_df[\n",
    "                          (evaluation_single_models_df[\"model\"] == model_name) & (evaluation_single_models_df[\"game\"] == game_name)\n",
    "                        ].sort_values(by=[\"mean\", \"standard_deviation\"], ascending=[False, True])[:N_FOR_TOP_N_ENSEMBLES]\n",
    "\n",
    "        model_paths.extend([str(Path(m_path).resolve()) for m_path in list(model_selection[\"model_path\"])])\n",
    "\n",
    "    models = load_models(model_paths)\n",
    "    for ensemble_method in ENSEMBLE_METHODS_USED:\n",
    "        t.set_description(f\"Evaluate mixed Ensemble ({ensemble_method}) for {game_name}\")\n",
    "        ensemble = Ensemble(models)\n",
    "        rewards = evaluate_ensemble(game_name, ensemble, num_repetitions=NUM_REPETITIONS)\n",
    "        evaluation_results_mixed_ensembles.append({\n",
    "            \"game\": game_name,\n",
    "            \"model\": f\"Top-{N_FOR_TOP_N_ENSEMBLES} Mixed Ensemble ({ensemble_method})\",\n",
    "            \"model_id\": 0,\n",
    "            \"episode_rewards\": rewards,\n",
    "            \"mean\": np.mean(rewards),\n",
    "            \"standard_deviation\": np.std(rewards),\n",
    "            \"model_path\": model_paths\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_mixed_ensembles_df = pd.DataFrame.from_records(evaluation_results_mixed_ensembles)\n",
    "evaluation_results_mixed_ensembles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Ensemble\n",
    "\n",
    "Ensembles consisting of the last $M$ training snapshots of a model. In the original paper, the snapshot get selected on the fly during training by saving models at local minima and increasing the learning rate after a model was selected. This leads to models that are more different from another and achieve higher results. This is not easily applicable for RL, so I use the $M$ newest snapshots instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_snapshot_ensembles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_name, model_name in (t := tqdm(list(itertools.product(list_of_games, models_used)),\n",
    "                                        desc=\"Build Snapshot Ensemble\")):\n",
    "    t.set_description(f\"Build Snapshot Ensemble ({model_name.capitalize()} for {game_name})\")\n",
    "\n",
    "    model_paths_for_game_and_model = \\\n",
    "        evaluation_single_models_df[(evaluation_single_models_df[\"model\"] == model_name) & (evaluation_single_models_df[\"game\"] == game_name)][\"model_path\"]\n",
    "\n",
    "    for idx, unique_model_path in enumerate(model_paths_for_game_and_model.unique()):\n",
    "        model_paths = get_snapshots(unique_model_path)\n",
    "        # Select M last snapshots:\n",
    "        model_paths = model_paths[-M_FOR_SNAPSHOT_ENSEMBLES_AND_SOUPS:]\n",
    "        models = load_models(model_paths)\n",
    "\n",
    "        for idx, ensemble_method in enumerate(ENSEMBLE_METHODS_USED):\n",
    "            ensemble = Ensemble(models, ensemble_method)\n",
    "\n",
    "            rewards = evaluate_ensemble(game_name, ensemble, NUM_REPETITIONS)\n",
    "            evaluation_results_snapshot_ensembles.append({\n",
    "                \"game\": game_name,\n",
    "                \"model\": f\"{M_FOR_SNAPSHOT_ENSEMBLES_AND_SOUPS}-Snapshot Ensemble ({ensemble_method}) with {model_name}\",\n",
    "                \"model_id\": idx,\n",
    "                \"episode_rewards\": rewards,\n",
    "                \"mean\": np.mean(rewards),\n",
    "                \"standard_deviation\": np.std(rewards),\n",
    "                \"model_path\": model_paths\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_snapshot_ensembles_df = pd.DataFrame.from_records(evaluation_results_snapshot_ensembles)\n",
    "evaluation_results_snapshot_ensembles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate as Soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_used = set(list_of_algorithms) - {\"random play\", \"interpretable_cnn\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Soups created from the best $n$ models of a flavor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_uniform_soups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Soup import Soup\n",
    "\n",
    "for game_name, model_name in (t := tqdm(list(itertools.product(list_of_games, models_used)))):\n",
    "    t.set_description(f\"Cooking the Soup Model ({model_name.capitalize()} for {game_name})\")\n",
    "\n",
    "    # Select Top-N models:\n",
    "    model_selection = evaluation_single_models_df[\n",
    "                          (evaluation_single_models_df[\"model\"] == model_name) & (evaluation_single_models_df[\"game\"] == game_name)\n",
    "                        ].sort_values(by=[\"mean\", \"standard_deviation\"], ascending=[False, True])[:N_FOR_TOP_N_ENSEMBLES]\n",
    "\n",
    "    model_paths = [str(Path(m_path).resolve()) for m_path in list(model_selection[\"model_path\"])]\n",
    "    models = load_models(model_paths)\n",
    "\n",
    "    soup = Soup(models).get_soup_model()\n",
    "\n",
    "    rewards = evaluate_model(game_name, soup, NUM_REPETITIONS)\n",
    "    evaluation_results_uniform_soups.append({\n",
    "        \"game\": game_name,\n",
    "        \"model\": f\"Top-{N_FOR_TOP_N_ENSEMBLES} Soup of {model_name}\",\n",
    "        \"model_id\": 0,\n",
    "        \"episode_rewards\": rewards,\n",
    "        \"mean\": np.mean(rewards),\n",
    "        \"standard_deviation\": np.std(rewards),\n",
    "        \"model_path\": model_paths\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_results_uniform_soups_df = pd.DataFrame.from_records(evaluation_results_uniform_soups)\n",
    "evaluation_results_uniform_soups_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Soup\n",
    "\n",
    "Works like the Snapshot Ensemble, but as a Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_snapshot_soups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_name, model_name in (t := tqdm(list(itertools.product(list_of_games, models_used)),\n",
    "                                        desc=\"Cooking Soup Model\")):\n",
    "    t.set_description(f\"Cooking Soup Model ({model_name.capitalize()} for {game_name})\")\n",
    "\n",
    "    model_paths_for_game_and_model = \\\n",
    "        results_df[(results_df[\"training_model\"] == model_name) & (results_df[\"game\"] == game_name)][\"model_path\"]\n",
    "\n",
    "    for idx, unique_model_path in enumerate(model_paths_for_game_and_model.unique()):\n",
    "        model_paths = get_snapshots(unique_model_path)\n",
    "        # Select M last snapshots:\n",
    "        model_paths = model_paths[-M_FOR_SNAPSHOT_ENSEMBLES_AND_SOUPS:]\n",
    "        models = load_models(model_paths)\n",
    "\n",
    "        soup = Soup(models).get_soup_model()\n",
    "\n",
    "        rewards = evaluate_model(game_name, soup, NUM_REPETITIONS)\n",
    "        evaluation_results_snapshot_soups.append({\n",
    "            \"game\": game_name,\n",
    "            \"model\": f\"{M_FOR_SNAPSHOT_ENSEMBLES_AND_SOUPS}-Snapshot Soup {model_name}\",\n",
    "            \"model_id\": idx,\n",
    "            \"episode_rewards\": rewards,\n",
    "            \"mean\": np.mean(rewards),\n",
    "            \"standard_deviation\": np.std(rewards),\n",
    "            \"model_path\": model_paths\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_snapshot_soups_df = pd.DataFrame.from_records(evaluation_results_snapshot_soups)\n",
    "evaluation_results_snapshot_soups_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data_df = pd.concat([\n",
    "    random_baseline_df,\n",
    "    evaluation_single_models_df,\n",
    "    # Ensembles:\n",
    "    evaluation_results_uniform_ensembles_df,\n",
    "    evaluation_results_mixed_ensembles_df,\n",
    "    evaluation_results_snapshot_ensembles_df,\n",
    "    # Soups:\n",
    "    evaluation_results_uniform_soups_df,\n",
    "    evaluation_results_snapshot_soups_df\n",
    "])\n",
    "evaluation_data_df.reset_index(drop=True, inplace=True)\n",
    "evaluation_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data_df.to_csv(EVALUATION_RESULTS_CSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
